{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Classification Tutorial {#vision__classification_tutorial}\n",
    "=============================\n",
    "\n",
    "In this tutorial, you will learn how to validate your **classification\n",
    "model** using deepchecks test suites. You can read more about the\n",
    "different checks and suites for computer vision use cases at the\n",
    "`examples section  <vision__checks_gallery>`{.interpreted-text\n",
    "role=\"ref\"}.\n",
    "\n",
    "A classification model is usually used to classify an image into one of\n",
    "a number of classes. Although there are multi label use-cases, in which\n",
    "the model is used to classify an image into multiple classes, most\n",
    "use-cases require the model to classify images into a single class.\n",
    "Currently, deepchecks supports only single label classification (either\n",
    "binary or multi-class).\n",
    "\n",
    "``` {.bash}\n",
    "# Before we start, if you don't have deepchecks vision package installed yet, run:\n",
    "import sys\n",
    "!{sys.executable} -m pip install \"deepchecks[vision]\" --quiet --upgrade # --user\n",
    "\n",
    "# or install using pip from your python environment\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the data and model\n",
    "===========================\n",
    "\n",
    "::: {.note}\n",
    "::: {.title}\n",
    "Note\n",
    ":::\n",
    "\n",
    "In this tutorial, we use the pytorch to create the dataset and model. To\n",
    "see how this can be done using tensorflow or other frameworks, please\n",
    "visit the\n",
    "`creating VisionData guide <vision__vision_data_class>`{.interpreted-text\n",
    "role=\"ref\"}.\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the dataset\n",
    "=======================\n",
    "\n",
    "The data is available from the torch library. We will download and\n",
    "extract it to the current directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "# url = 'https://download.pytorch.org/tutorial/hymenoptera_data.zip'\n",
    "# urllib.request.urlretrieve(url, './hymenoptera_data.zip')\n",
    "\n",
    "# with zipfile.ZipFile('./hymenoptera_data.zip', 'r') as zip_ref:\n",
    "#     zip_ref.extractall('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data\n",
    "=========\n",
    "\n",
    "We will use torchvision and torch.utils.data packages for loading the\n",
    "data. The model we are building will learn to classify **ants** and\n",
    "**bees**. We have about 120 training images each for ants and bees.\n",
    "There are 75 validation images for each class. This dataset is a very\n",
    "small subset of imagenet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import torch\n",
    "import torchvision\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class AntsBeesDataset(torchvision.datasets.ImageFolder):\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \"\"\"overrides __getitem__ to be compatible to albumentations\"\"\"\n",
    "        path, target = self.samples[index]\n",
    "        sample = self.loader(path)\n",
    "        sample = self.get_cv2_image(sample)\n",
    "        if self.transforms is not None:\n",
    "            transformed = self.transforms(image=sample, target=target)\n",
    "            sample, target = transformed[\"image\"], transformed[\"target\"]\n",
    "        else:\n",
    "            if self.transform is not None:\n",
    "                sample = self.transform(image=sample)['image']\n",
    "            if self.target_transform is not None:\n",
    "                target = self.target_transform(target)\n",
    "\n",
    "        return sample, target\n",
    "\n",
    "    def get_cv2_image(self, image):\n",
    "        if isinstance(image, PIL.Image.Image):\n",
    "            return np.array(image).astype('uint8')\n",
    "        elif isinstance(image, np.ndarray):\n",
    "            return image\n",
    "        else:\n",
    "            raise RuntimeError(\"Only PIL.Image and CV2 loaders currently supported!\")\n",
    "\n",
    "data_dir = './hymenoptera_data'\n",
    "# Just normalization for validation\n",
    "data_transforms = A.Compose([\n",
    "    A.Resize(height=256, width=256),\n",
    "    A.CenterCrop(height=224, width=224),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "train_dataset = AntsBeesDataset(root=os.path.join(data_dir,'train'))\n",
    "train_dataset.transforms = data_transforms\n",
    "\n",
    "test_dataset = AntsBeesDataset(root=os.path.join(data_dir, 'val'))\n",
    "test_dataset.transforms = data_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the dataset\n",
    "=====================\n",
    "\n",
    "Let\\'s see how our data looks like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 244\n",
      "Number of validation images: 153\n",
      "Example output of an image shape: torch.Size([3, 224, 224])\n",
      "Example output of a label: 0\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training images: {len(train_dataset)}')\n",
    "print(f'Number of validation images: {len(test_dataset)}')\n",
    "print(f'Example output of an image shape: {train_dataset[0][0].shape}')\n",
    "print(f'Example output of a label: {train_dataset[0][1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading a pre-trained model\n",
    "===============================\n",
    "\n",
    "Now, we will download a pre-trained model from torchvision, that was\n",
    "trained on the ImageNet dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vishesh/Desktop/perturbations/venv/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/vishesh/Desktop/perturbations/venv/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "# We have only 2 classes\n",
    "model.fc = nn.Linear(num_ftrs, 2)\n",
    "model = model.to(device)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validating the Model with Deepchecks\n",
    "====================================\n",
    "\n",
    "Now, after we have the training data, validation data and the model, we\n",
    "can validate the model with deepchecks test suites.\n",
    "\n",
    "Implementing the VisionData class\n",
    "---------------------------------\n",
    "\n",
    "The checks in the package validate the model & data by calculating\n",
    "various quantities over the data, labels and predictions. In order to do\n",
    "that, those must be in a pre-defined format, according to the task type.\n",
    "In the following example we\\'re using pytorch. To see an implementation\n",
    "of this in tensorflow, please refer to the\n",
    "`vision__vision_data_class`{.interpreted-text role=\"ref\"} guide. For\n",
    "pytorch, we will use our DataLoader, but we\\'ll create a new collate\n",
    "function for it, that transforms the batch to the correct format. Then,\n",
    "we\\'ll create a\n",
    "`deepchecks.vision.vision_data.vision_data.VisionData`{.interpreted-text\n",
    "role=\"class\"} object, that will hold the data loader.\n",
    "\n",
    "To learn more about the expected formats, please visit the\n",
    "`vision__supported_tasks`{.interpreted-text role=\"ref\"}.\n",
    "\n",
    "First, we\\'ll create the collate function that will be used by the\n",
    "DataLoader. In pytorch, the collate function is used to transform the\n",
    "output batch to any custom format, and we\\'ll use that in order to\n",
    "transform the batch to the correct format for the checks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.vision.vision_data import BatchOutputFormat\n",
    "\n",
    "def deepchecks_collate_fn(batch) -> BatchOutputFormat:\n",
    "    \"\"\"Return a batch of images, labels and predictions for a batch of data. The expected format is a dictionary with\n",
    "    the following keys: 'images', 'labels' and 'predictions', each value is in the deepchecks format for the task.\n",
    "    You can also use the BatchOutputFormat class to create the output.\n",
    "    \"\"\"\n",
    "    # batch received as iterable of tuples of (image, label) and transformed to tuple of iterables of images and labels:\n",
    "    batch = tuple(zip(*batch))\n",
    "\n",
    "    # images:\n",
    "    inp = torch.stack(batch[0]).detach().numpy().transpose((0, 2, 3, 1))\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    inp = std * inp + mean\n",
    "    images = np.clip(inp, 0, 1) * 255\n",
    "\n",
    "    #labels:\n",
    "    labels = batch[1]\n",
    "\n",
    "    #predictions:\n",
    "    logits = model.to(device)(torch.stack(batch[0]).to(device))\n",
    "    predictions = nn.Softmax(dim=1)(logits)\n",
    "    return BatchOutputFormat(images=images, labels=labels, predictions=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a single label here, which is the tomato class The label\\_map is\n",
    "a dictionary that maps the class id to the class name, for display\n",
    "purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_MAP = {\n",
    "    0: 'ants',\n",
    "    1: 'bees'\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our updated collate function, we can recreate the\n",
    "dataloader in the deepchecks format, and use it to create a VisionData\n",
    "object:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.vision import VisionData\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=deepchecks_collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=True, collate_fn=deepchecks_collate_fn)\n",
    "\n",
    "training_data = VisionData(batch_loader=train_loader, task_type='classification', label_map=LABEL_MAP)\n",
    "test_data = VisionData(batch_loader=test_loader, task_type='classification', label_map=LABEL_MAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making sure our data is in the correct format:\n",
    "==============================================\n",
    "\n",
    "The VisionData object automatically validates your data format and will\n",
    "alert you if there is a problem. However, you can also manually view\n",
    "your images and labels to make sure they are in the correct format by\n",
    "using the `head` function to conveniently visualize your data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae57cf0ef4654428b3148c8ae44358d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<div style=\"display:flex; flex-direction: column; gap: 10px;\">\\n                <di…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And observe the output:\n",
    "\n",
    "Running Deepchecks\\' suite on our data and model!\n",
    "=================================================\n",
    "\n",
    "Now that we have defined the task class, we can validate the train and\n",
    "test data with deepchecks\\' train test validation suite. This can be\n",
    "done with this simple few lines of code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        progress {\n",
       "            -webkit-appearance: none;\n",
       "            border: none;\n",
       "            border-radius: 3px;\n",
       "            width: 300px;\n",
       "            height: 20px;\n",
       "            vertical-align: middle;\n",
       "            margin-right: 10px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-bar {\n",
       "            border-radius: 3px;\n",
       "            background-color: aliceblue;\n",
       "        }\n",
       "        progress::-webkit-progress-value {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "        progress::-moz-progress-bar {\n",
       "            background-color: #9d60fb;\n",
       "        }\n",
       "    </style>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepchecks.vision.suites import train_test_validation\n",
    "\n",
    "suite = train_test_validation()\n",
    "result = suite.run(training_data, test_data,  max_samples = 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have suites for:\n",
    "`data integrity <deepchecks.vision.suites.data_integrity>`{.interpreted-text\n",
    "role=\"func\"} - validating a single dataset and\n",
    "`model evaluation <deepchecks.vision.suites.model_evaluation>`{.interpreted-text\n",
    "role=\"func\"} -evaluating the model\\'s performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the results:\n",
    "======================\n",
    "\n",
    "The results can be saved as a html file with the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deepchecks.core.suite.SuiteResult"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output.html'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.save_as_html('output.html')\n",
    "\n",
    "# Or displayed in a new window in an IDE like Pycharm:\n",
    "# result.show_in_window()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, if working inside a notebook, the output can be displayed directly\n",
    "by simply printing the result object:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "949b3b43db9448bfb5f529353cef5d1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(HTML(value='\\n<h1 id=\"summary_R4KZ36U8UEIHX74D0GJYV5G10\">Train Test Validat…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we do not have any meaningful issues with our data, and\n",
    "although there\\'s some drift between the train and test datasets (under\n",
    "the \\\"Passed\\\" section), this is not significant enough to cause any\n",
    "issues (and therefor is not displayed in the \\\"Didn\\'t Pass\\\" section).\n",
    "However, under the \\\"Other\\\" section, that details checks without a\n",
    "specific pass/fail condition, we can see that the heatmap of brightness\n",
    "in the images is not uniformly distributed, which means that in most\n",
    "images, there are brighter objects in the center of the image. This\n",
    "makes sense as these images of bees and ants tend to have the insects in\n",
    "the center of the image, but it is something to be aware of and maybe\n",
    "use data augmentation to fix.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
